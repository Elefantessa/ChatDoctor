# Training Configuration for ChatDoctor LoRA

model:
  base_model: "./models/llama-base"
  lora_adapter: null
  torch_dtype: "float16"
  device_map: "auto"
  load_in_4bit: true

training:
  output_dir: "./models/lora_trained"
  data_path: "./data/HealthCareMagic-100k.json"
  batch_size: 32
  micro_batch_size: 2
  num_epochs: 1
  learning_rate: 3.0e-4
  cutoff_len: 256
  val_set_size: 200
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  gradient_checkpointing: true
  use_flash_attention: false

inference:
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9

logging:
  level: "INFO"

wandb:
  project: "chatdoctor-lora"
  run_name: "HealthCareMagic-100k-training"
