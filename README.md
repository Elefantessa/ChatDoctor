# ğŸ©º ChatDoctor v2

> **Fine-tuned Medical AI Assistant with State-of-the-Art Performance**

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](LICENSE)

A production-ready medical chatbot fine-tuned on **111,000+ real doctor-patient conversations**, achieving **BERTScore F1 = 0.844** â€” matching the original ChatDoctor paper results.

---

## ğŸ¯ Key Results

### Performance Comparison with Original Paper

| Model | Precision | Recall | F1 Score | Source |
|-------|-----------|--------|----------|--------|
| ChatGPT | 0.837 | 0.845 | 0.841 | Paper |
| ChatDoctor (Paper) | 0.844 | 0.845 | 0.841 | Paper |
| **ChatDoctor v2 (Ours)** | **0.845** | **0.843** | **0.844** | This Project |

âœ… **Our implementation matches/exceeds the original paper performance!**

---

## ğŸ”„ What's New in v2? (Improvements over Original)

### Architecture Changes

| Aspect | Original ChatDoctor | ChatDoctor v2 |
|--------|-------------------|---------------|
| **Base Model** | LLaMA-1 7B | Mistral-7B-Instruct-v0.3 |
| **Quantization** | 8-bit | 4-bit (QLoRA) |
| **Training Method** | LoRA | QLoRA with gradient checkpointing |
| **Codebase** | Single scripts | Modular Python package |
| **OpenAI API** | Legacy v0.x | Modern v1.x |
| **Configuration** | Hardcoded | YAML-based configs |
| **Evaluation** | Basic | BERTScore + BLEU + ROUGE |

### Key Improvements

1. **ğŸš€ Upgraded Base Model**: Mistral-7B outperforms LLaMA on all benchmarks
2. **âš¡ Memory Efficient**: 4-bit quantization reduces VRAM by 50%
3. **ğŸ“¦ Modular Design**: Clean separation of concerns (core, training, inference, rag)
4. **ğŸ”§ Modern Dependencies**: Updated to latest transformers, peft, bitsandbytes
5. **ğŸ“Š Comprehensive Evaluation**: Added BERTScore as per paper methodology
6. **ğŸ§ª Unit Tests**: Test coverage for config and data utilities
7. **ğŸ“ Better Documentation**: Detailed README with usage examples

---

## ğŸ“ Training Process

### Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HealthCareMagic â”‚â”€â”€â”€â”€â–¶â”‚  QLoRA Training  â”‚â”€â”€â”€â”€â–¶â”‚  LoRA Adapter   â”‚
â”‚   100k Dataset  â”‚     â”‚  (Mistral-7B)    â”‚     â”‚    (161 MB)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     111,665                7.5 hours                Final Model
     samples               on 2x A100
```

### Training Configuration

```python
# QLoRA Configuration
base_model = "mistralai/Mistral-7B-Instruct-v0.3"
quantization = "4-bit (nf4)"
lora_r = 16
lora_alpha = 32
lora_dropout = 0.05
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Training Hyperparameters
batch_size = 64
micro_batch_size = 2
gradient_accumulation_steps = 32
learning_rate = 2e-4
num_epochs = 1
warmup_steps = 100
max_seq_length = 512
```

### Training Command

```bash
python -m chatdoctor.training.train_lora \
  --base_model ./models/mistral \
  --data_path ./data/HealthCareMagic-100k.json \
  --output_dir ./models/mistral_lora \
  --batch_size 64 \
  --micro_batch_size 2 \
  --num_epochs 1 \
  --learning_rate 2e-4 \
  --lora_r 16 \
  --lora_alpha 32 \
  --use_4bit True
```

### Training Results

| Metric | Value |
|--------|-------|
| Training Samples | 111,665 |
| Validation Samples | 500 |
| Total Steps | 1,745 |
| Training Time | 7h 26m |
| Initial Loss | ~2.0 |
| Final Loss | 1.50 |
| GPU Usage | 2x NVIDIA A100 80GB |

### Loss Curve

```
Loss
2.0 â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚â•²
1.8 â”€â”¤ â•²
     â”‚  â•²
1.6 â”€â”¤   â•²__
     â”‚      â•²___
1.4 â”€â”¤          â•²_______
     â”‚                  â•²___________
1.2 â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Steps
     0    400   800   1200  1600  1745
```

---

## ğŸ“Š Evaluation Methodology

We use **BERTScore** as per the original paper:

> "BERTScore leverages pre-trained BERT to match words in the candidate and reference sentences via cosine similarity... chosen for its ability to evaluate the semantic similarity between our model's responses and the reference sentences."
> â€” ChatDoctor Paper (Li et al., 2023)

### Evaluation Script

```bash
# BERTScore evaluation (paper methodology)
python -m chatdoctor.evaluation.evaluate_bertscore \
  --model_path ./models/mistral \
  --lora_path ./models/mistral_lora \
  --data_path ./data/icliniq.json \
  --max_samples 50
```

### Full Evaluation Results

| Model | BERTScore P | BERTScore R | BERTScore F1 | BLEU-1 | ROUGE-L |
|-------|-------------|-------------|--------------|--------|---------|
| LLaMA-2 Base | 0.826 | 0.828 | 0.827 | 0.125 | 0.103 |
| LLaMA-2 + LoRA | 0.826 | 0.828 | 0.827 | 0.137 | 0.102 |
| Mistral Base | - | - | - | 0.141 | 0.115 |
| **Mistral + LoRA** | **0.845** | **0.843** | **0.844** | **0.150** | 0.109 |

---

## ğŸ—ï¸ Project Structure

```
chatdoctor_v2/
â”œâ”€â”€ chatdoctor/                 # Main Python package
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # Dataclass-based configuration
â”‚   â”‚   â””â”€â”€ model.py           # ChatDoctorModel with 4-bit loading
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ train_lora.py      # QLoRA fine-tuning script
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ chat.py            # Interactive chat interface
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluate.py        # BLEU/ROUGE metrics
â”‚   â”‚   â”œâ”€â”€ evaluate_icliniq.py # Token-level P/R/F1
â”‚   â”‚   â””â”€â”€ evaluate_bertscore.py # BERTScore (paper method)
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ csv_rag.py         # Disease database RAG
â”‚   â”‚   â””â”€â”€ wiki_rag.py        # Wikipedia RAG
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_utils.py      # JSON/data handling
â”‚       â””â”€â”€ logging_utils.py   # Rich console logging
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mistral/               # Mistral-7B-Instruct (14GB)
â”‚   â””â”€â”€ mistral_lora/          # Fine-tuned adapter (161MB)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ HealthCareMagic-100k.json  # Training data (111K)
â”‚   â”œâ”€â”€ chatdoctor5k.json          # Evaluation data (5.4K)
â”‚   â””â”€â”€ icliniq.json               # BERTScore eval (7.3K)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml            # Main system config
â”‚   â””â”€â”€ training_config.yaml   # Training parameters
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ chat.sh               # Interactive chat
â”‚   â”œâ”€â”€ train.sh              # Training script
â”‚   â””â”€â”€ evaluate.sh           # Evaluation script
â””â”€â”€ tests/                    # Unit tests
```

---

## ğŸ“¦ Installation

### 1. Clone Repository

```bash
git clone https://github.com/Elefantessa/ChatDoctor.git
cd ChatDoctor
pip install -e .
```

### 2. Download Base Models

Choose one of the following base models:

**Mistral-7B (Recommended):**
```bash
# Option A: Using huggingface-cli
pip install huggingface-hub
huggingface-cli download mistralai/Mistral-7B-Instruct-v0.3 --local-dir ./models/mistral

# Option B: Using Python
python -c "
from huggingface_hub import snapshot_download
snapshot_download('mistralai/Mistral-7B-Instruct-v0.3', local_dir='./models/mistral')
"
```

**LLaMA-3 8B:**
```bash
huggingface-cli download NousResearch/Meta-Llama-3-8B-Instruct --local-dir ./models/llama3-8b
```

### 3. Download LoRA Adapters

The fine-tuned LoRA adapters are hosted on HuggingFace:

```bash
# Mistral LoRA (161MB) - BERTScore F1 = 0.844
huggingface-cli download Elefantessa/chatdoctor-mistral-lora --local-dir ./models/mistral_lora

# Or download manually from:
# https://huggingface.co/Elefantessa/chatdoctor-mistral-lora
```

### 4. Download Training Data (Optional)

```bash
# HealthCareMagic-100k (Training)
python -c "
from datasets import load_dataset
ds = load_dataset('lavita/ChatDoctor-HealthCareMagic-100k')
import json
with open('./data/HealthCareMagic-100k.json', 'w') as f:
    json.dump([dict(x) for x in ds['train']], f)
"

# iCliniq (Evaluation)
python -c "
from datasets import load_dataset
ds = load_dataset('lavita/ChatDoctor-iCliniq')
import json
with open('./data/icliniq.json', 'w') as f:
    json.dump([dict(x) for x in ds['train']], f)
"
```

---

## ğŸš€ Quick Start

```bash
# Chat with the best model (Mistral + LoRA)
./scripts/chat.sh --mistral

# Chat with knowledge retrieval
./scripts/chat.sh --mistral --rag csv

# Run BERTScore evaluation
./scripts/evaluate.sh --bertscore
```

### Example Conversation

```
Patient: I have headache and fever for 2 days. What should I do?

ChatDoctor: Hi, Thanks for your query. Based on your symptoms, you may have a
viral infection. I recommend:

1. Tab Paracetamol 500mg every 6 hours for fever
2. Tab Ibuprofen 400mg for headache relief
3. Drink plenty of fluids and rest
4. If fever exceeds 103Â°F or symptoms worsen, please visit a doctor

Hope this helps. Take care.
```

---

## ğŸ“ˆ Datasets

| Dataset | Samples | Purpose | Source |
|---------|---------|---------|--------|
| HealthCareMagic-100k | 111,665 | Training | [HuggingFace](https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k) |
| ChatDoctor-5k | 5,452 | BLEU/ROUGE eval | Original ChatDoctor |
| iCliniq | 7,321 | BERTScore eval | [HuggingFace](https://huggingface.co/datasets/lavita/ChatDoctor-iCliniq) |

---

## ğŸ“š References

- [ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge](https://arxiv.org/abs/2303.14070) - Li et al., 2023
- [Mistral 7B](https://arxiv.org/abs/2310.06825) - Mistral AI, 2023
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) - Dettmers et al., 2023
- [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675) - Zhang et al., 2020

---

## âš ï¸ Disclaimer

This project is for **research and educational purposes only**. Do not use for actual medical diagnosis. Always consult qualified healthcare professionals.

---

## ğŸ“„ License

Apache 2.0 License

---

<p align="center">
  <b>Built with â¤ï¸ for Medical AI Research</b>
</p>
